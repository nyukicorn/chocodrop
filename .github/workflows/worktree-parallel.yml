name: Worktree Parallel Testing

on:
  workflow_dispatch:
    inputs:
      branches:
        description: 'Comma-separated branch names to test (e.g., task/feature-a,task/feature-b,task/feature-c)'
        required: true
        type: string
      run_build:
        description: 'Run build for each branch'
        required: false
        type: boolean
        default: true
      run_tests:
        description: 'Run tests for each branch'
        required: false
        type: boolean
        default: true
      compare_results:
        description: 'Generate comparison report'
        required: false
        type: boolean
        default: true

permissions:
  contents: read
  pull-requests: write
  issues: write
  id-token: write

jobs:
  prepare:
    name: Prepare branch matrix
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      branch_count: ${{ steps.set-matrix.outputs.count }}

    steps:
      - name: Parse branches
        id: set-matrix
        run: |
          BRANCHES="${{ github.event.inputs.branches }}"
          # カンマ区切りを配列に変換
          IFS=',' read -ra BRANCH_ARRAY <<< "$BRANCHES"

          # スペースをトリム
          CLEANED_BRANCHES=()
          for branch in "${BRANCH_ARRAY[@]}"; do
            trimmed=$(echo "$branch" | xargs)
            CLEANED_BRANCHES+=("\"$trimmed\"")
          done

          # JSON配列を作成
          BRANCH_JSON="[${CLEANED_BRANCHES[*]}]"
          BRANCH_JSON="${BRANCH_JSON// /,}"

          echo "matrix=$BRANCH_JSON" >> $GITHUB_OUTPUT
          echo "count=${#BRANCH_ARRAY[@]}" >> $GITHUB_OUTPUT

          echo "🌳 Testing ${#BRANCH_ARRAY[@]} branches:"
          for branch in "${BRANCH_ARRAY[@]}"; do
            echo "  - $(echo $branch | xargs)"
          done

  # 各ブランチを並行実行
  test-branch:
    name: Test ${{ matrix.branch }}
    runs-on: ubuntu-latest
    needs: prepare
    strategy:
      fail-fast: false
      matrix:
        branch: ${{ fromJson(needs.prepare.outputs.matrix) }}
        node: [20]

    steps:
      - name: Checkout branch ${{ matrix.branch }}
        uses: actions/checkout@v4
        with:
          ref: ${{ matrix.branch }}
          fetch-depth: 0

      - name: Setup Node.js ${{ matrix.node }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node }}
          cache: 'npm'

      - name: Get branch info
        id: branch_info
        run: |
          BRANCH_NAME="${{ matrix.branch }}"
          COMMIT_SHA=$(git rev-parse --short HEAD)
          COMMIT_MSG=$(git log -1 --pretty=%s)
          BRANCH_SAFE=$(echo "$BRANCH_NAME" | sed 's/[^a-zA-Z0-9]/-/g')

          echo "branch_safe=$BRANCH_SAFE" >> $GITHUB_OUTPUT
          echo "commit_sha=$COMMIT_SHA" >> $GITHUB_OUTPUT
          echo "commit_msg=$COMMIT_MSG" >> $GITHUB_OUTPUT

          echo "📋 Branch: $BRANCH_NAME"
          echo "📝 Commit: $COMMIT_SHA - $COMMIT_MSG"

      - name: Install dependencies
        run: |
          echo "📦 Installing dependencies for ${{ matrix.branch }}..."
          npm ci

      - name: Run linting
        id: lint
        run: |
          echo "🔍 Running linting..."
          npm run lint > lint-output.txt 2>&1 || true
          cat lint-output.txt

          if grep -q "error" lint-output.txt; then
            echo "status=failed" >> $GITHUB_OUTPUT
          else
            echo "status=passed" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Run build
        id: build
        if: github.event.inputs.run_build == 'true'
        run: |
          echo "🔨 Building ${{ matrix.branch }}..."
          START_TIME=$(date +%s)

          npm run build > build-output.txt 2>&1 || BUILD_FAILED=1

          END_TIME=$(date +%s)
          BUILD_TIME=$((END_TIME - START_TIME))

          echo "duration=$BUILD_TIME" >> $GITHUB_OUTPUT

          if [ "$BUILD_FAILED" == "1" ]; then
            echo "status=failed" >> $GITHUB_OUTPUT
            cat build-output.txt
            exit 1
          else
            echo "status=passed" >> $GITHUB_OUTPUT
            echo "✅ Build completed in ${BUILD_TIME}s"
          fi
        continue-on-error: true

      - name: Run tests
        id: test
        if: github.event.inputs.run_tests == 'true'
        run: |
          echo "🧪 Running tests for ${{ matrix.branch }}..."
          START_TIME=$(date +%s)

          npm test > test-output.txt 2>&1 || TEST_FAILED=1

          END_TIME=$(date +%s)
          TEST_TIME=$((END_TIME - START_TIME))

          echo "duration=$TEST_TIME" >> $GITHUB_OUTPUT

          if [ "$TEST_FAILED" == "1" ]; then
            echo "status=failed" >> $GITHUB_OUTPUT
            cat test-output.txt
          else
            echo "status=passed" >> $GITHUB_OUTPUT
            echo "✅ Tests completed in ${TEST_TIME}s"
          fi

          # テスト結果をカウント
          PASS_COUNT=$(grep -c "# pass" test-output.txt || echo "0")
          FAIL_COUNT=$(grep -c "# fail" test-output.txt || echo "0")

          echo "pass_count=$PASS_COUNT" >> $GITHUB_OUTPUT
          echo "fail_count=$FAIL_COUNT" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Start dev server for dynamic evaluation
        id: dev_server
        run: |
          echo "🚀 Starting dev server for visual and performance evaluation..."
          npm run dev &
          DEV_PID=$!
          echo "pid=$DEV_PID" >> $GITHUB_OUTPUT

          # Wait for server to be ready
          npx wait-on http://localhost:8000 --timeout 60000 || echo "Server timeout"
        continue-on-error: true

      - name: Install Playwright
        run: |
          npx playwright install --with-deps chromium

      - name: Take screenshots
        id: screenshots
        run: |
          mkdir -p screenshots

          echo "📸 Taking screenshots of examples..."

          # ChocoDrop examples
          EXAMPLES=("basic" "lofi-room" "lofi-city" "music-garden" "space" "toy-city" "wabi-sabi")

          for example in "${EXAMPLES[@]}"; do
            # Desktop
            npx playwright screenshot \
              "http://localhost:8000/examples/$example/" \
              "screenshots/$example-desktop.png" \
              --viewport-size=1920,1080 \
              --timeout=30000 || echo "Screenshot failed: $example desktop"

            # Mobile
            npx playwright screenshot \
              "http://localhost:8000/examples/$example/" \
              "screenshots/$example-mobile.png" \
              --viewport-size=375,667 \
              --timeout=30000 || echo "Screenshot failed: $example mobile"
          done

          echo "status=completed" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Run Lighthouse performance audit
        id: lighthouse
        run: |
          echo "⚡ Running Lighthouse performance audit..."
          npm install -g lighthouse

          # Test key examples
          KEY_EXAMPLES=("basic" "lofi-room" "music-garden")

          for example in "${KEY_EXAMPLES[@]}"; do
            lighthouse "http://localhost:8000/examples/$example/" \
              --output json \
              --output-path "performance-$example.json" \
              --chrome-flags="--headless" \
              --only-categories=performance \
              --preset=desktop \
              --quiet || echo "Lighthouse failed: $example"
          done

          echo "status=completed" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Stop dev server
        if: always()
        run: |
          if [ -n "${{ steps.dev_server.outputs.pid }}" ]; then
            kill ${{ steps.dev_server.outputs.pid }} || true
          fi
          # Kill any remaining node processes
          pkill -f "npm run dev" || true

      - name: Generate branch report
        run: |
          REPORT_DIR="reports/${{ steps.branch_info.outputs.branch_safe }}"
          mkdir -p "$REPORT_DIR"

          # Get branch statistics
          TOTAL_COMMITS=$(git rev-list --count HEAD)
          TOTAL_FILES=$(git ls-tree -r --name-only HEAD | wc -l | xargs)

          # Get bundle size if build succeeded
          BUNDLE_SIZE="0"
          if [ "${{ steps.build.outputs.status }}" == "passed" ] && [ -d "dist" ]; then
            BUNDLE_SIZE=$(du -sk dist | cut -f1)
          fi

          cat > "$REPORT_DIR/summary.json" << EOF
          {
            "branch": "${{ matrix.branch }}",
            "commit": "${{ steps.branch_info.outputs.commit_sha }}",
            "commit_message": "${{ steps.branch_info.outputs.commit_msg }}",
            "total_commits": "$TOTAL_COMMITS",
            "total_files": "$TOTAL_FILES",
            "bundle_size_kb": "$BUNDLE_SIZE",
            "node_version": "${{ matrix.node }}",
            "lint_status": "${{ steps.lint.outputs.status }}",
            "build_status": "${{ steps.build.outputs.status }}",
            "build_duration": "${{ steps.build.outputs.duration }}",
            "test_status": "${{ steps.test.outputs.status }}",
            "test_duration": "${{ steps.test.outputs.duration }}",
            "test_pass": "${{ steps.test.outputs.pass_count }}",
            "test_fail": "${{ steps.test.outputs.fail_count }}",
            "screenshots_status": "${{ steps.screenshots.outputs.status }}",
            "lighthouse_status": "${{ steps.lighthouse.outputs.status }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "evaluation_note": "This represents the final state of the branch (all commits, all files), not individual commits"
          }
          EOF

          # Markdown レポート生成
          cat > "$REPORT_DIR/report.md" << EOF
          # Test Report: ${{ matrix.branch }}

          ## Branch Information
          - **Branch**: \`${{ matrix.branch }}\`
          - **Commit**: \`${{ steps.branch_info.outputs.commit_sha }}\`
          - **Message**: ${{ steps.branch_info.outputs.commit_msg }}
          - **Node.js**: ${{ matrix.node }}

          ## Results

          | Check | Status | Duration |
          |-------|--------|----------|
          | Lint | ${{ steps.lint.outputs.status == 'passed' && '✅ Passed' || '❌ Failed' }} | - |
          | Build | ${{ steps.build.outputs.status == 'passed' && '✅ Passed' || steps.build.outputs.status == 'failed' && '❌ Failed' || '⏭️ Skipped' }} | ${{ steps.build.outputs.duration }}s |
          | Tests | ${{ steps.test.outputs.status == 'passed' && '✅ Passed' || steps.test.outputs.status == 'failed' && '❌ Failed' || '⏭️ Skipped' }} | ${{ steps.test.outputs.duration }}s |

          ### Test Details
          - **Passed**: ${{ steps.test.outputs.pass_count }}
          - **Failed**: ${{ steps.test.outputs.fail_count }}
          EOF

          cat "$REPORT_DIR/report.md"

      - name: Upload branch artifacts
        uses: actions/upload-artifact@v4
        with:
          name: report-${{ steps.branch_info.outputs.branch_safe }}
          path: reports/
          retention-days: 30

      - name: Upload dynamic evaluation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dynamic-eval-${{ steps.branch_info.outputs.branch_safe }}
          path: |
            screenshots/
            performance-*.json
          retention-days: 30
        continue-on-error: true

  # 全ブランチの結果を比較
  compare:
    name: Compare results
    runs-on: ubuntu-latest
    needs: [prepare, test-branch]
    if: github.event.inputs.compare_results == 'true'
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all reports
        uses: actions/download-artifact@v4
        with:
          path: all-reports

      - name: Generate comparison report
        run: |
          mkdir -p comparison

          cat > comparison/README.md << 'EOF'
          # Worktree Parallel Testing - Comparison Report

          ## Overview
          This report compares the results of parallel testing across multiple branches.

          ## Tested Branches
          EOF

          echo "Testing ${{ needs.prepare.outputs.branch_count }} branches" >> comparison/README.md
          echo "" >> comparison/README.md

          # 各ブランチのレポートを統合
          cat >> comparison/README.md << 'EOF'
          ## Comparison Table

          | Branch | Commit | Lint | Build | Build Time | Tests | Test Time | Pass | Fail |
          |--------|--------|------|-------|------------|-------|-----------|------|------|
          EOF

          for report_dir in all-reports/report-*/; do
            if [ -f "$report_dir/*/summary.json" ]; then
              JSON_FILE=$(find "$report_dir" -name "summary.json" | head -1)

              BRANCH=$(jq -r '.branch' "$JSON_FILE")
              COMMIT=$(jq -r '.commit' "$JSON_FILE")
              LINT=$(jq -r '.lint_status' "$JSON_FILE")
              BUILD=$(jq -r '.build_status' "$JSON_FILE")
              BUILD_DUR=$(jq -r '.build_duration' "$JSON_FILE")
              TEST=$(jq -r '.test_status' "$JSON_FILE")
              TEST_DUR=$(jq -r '.test_duration' "$JSON_FILE")
              PASS=$(jq -r '.test_pass' "$JSON_FILE")
              FAIL=$(jq -r '.test_fail' "$JSON_FILE")

              LINT_ICON=$([[ "$LINT" == "passed" ]] && echo "✅" || echo "❌")
              BUILD_ICON=$([[ "$BUILD" == "passed" ]] && echo "✅" || [[ "$BUILD" == "failed" ]] && echo "❌" || echo "⏭️")
              TEST_ICON=$([[ "$TEST" == "passed" ]] && echo "✅" || [[ "$TEST" == "failed" ]] && echo "❌" || echo "⏭️")

              echo "| $BRANCH | \`$COMMIT\` | $LINT_ICON | $BUILD_ICON | ${BUILD_DUR}s | $TEST_ICON | ${TEST_DUR}s | $PASS | $FAIL |" >> comparison/README.md
            fi
          done

          cat >> comparison/README.md << 'EOF'

          ## Recommendations

          Based on the test results:
          1. ✅ Branches with all tests passing are ready for merge
          2. ⚠️  Branches with build failures need attention
          3. ❌ Branches with test failures require fixes

          ## Next Steps
          1. Review failed branches
          2. Fix any issues
          3. Choose the best implementation
          4. Merge to main branch
          EOF

          cat comparison/README.md

      - name: 🤖 AI Code Comparison with Claude
        uses: anthropics/claude-code-action@beta
        with:
          mode: agent
          direct_prompt: |
            # 🚀 複数ブランチの最終成果物比較評価タスク（改善版 v2.0）

            あなたは経験豊富なシニアエンジニアです。複数のブランチを**最終成果物**として深く分析し、最良の選択を提案してください。

            ## 🎯 重要な前提条件

            **各ブランチは並列実装されており、途中のコミットは異なります。**
            **❌ コミット単位で比較しないでください。**
            **✅ 各ブランチの最終状態（全ファイル・全機能）を評価してください。**

            ---

            ## 📋 タスク概要

            `all-reports/` ディレクトリに以下のデータが保存されています：
            - テスト結果（`summary.json`）
            - スクリーンショット（`screenshots/`）
            - パフォーマンスデータ（`performance-*.json`）

            これらを**全て確認**し、総合的な比較評価を行ってください。

            ---

            ## 🔍 Phase 1: ブランチの最終成果物を把握

            ### 1-1. テスト結果の読み込み
            - `all-reports/report-*/reports/*/summary.json` を全て読み込む
            - ビルド時間、テスト成功率、バンドルサイズを把握
            - **注意**: `evaluation_note` に「最終状態を評価」と明記されています

            ### 1-2. ブランチ全体のコードを読み込む（必須 🚨）

            **各ブランチについて**、以下を実施してください：

            a. **プロジェクト構成の把握**
            ```bash
            # ブランチのファイル一覧を取得
            git ls-tree -r --name-only <branch>
            ```

            b. **主要ディレクトリを全て確認**
            - `src/` - ソースコード全体を読む
            - `examples/` - 実装例・デモを確認
            - `test/` - テストコードを確認
            - `package.json` - 依存関係を確認
            - その他設定ファイル

            c. **実装内容の全体像を把握**
            - どんな機能が実装されているか（全て列挙）
            - どんなファイル構成になっているか
            - どんな設計パターンを使っているか
            - どんな外部ライブラリを使っているか

            **❌ やってはいけないこと**:
            - コミット単位で比較する
            - 「変更ファイル」だけを見る
            - `git diff` だけを見る

            **✅ やるべきこと**:
            - ブランチ全体のファイルを読む
            - 実装されている機能を全て列挙
            - 最終成果物として評価

            ### 1-3. ビジュアル評価データの確認
            - `all-reports/dynamic-eval-*/screenshots/` のスクリーンショットを確認
            - デスクトップ版とモバイル版を比較
            - デザインの一貫性、レイアウト、配色を評価

            ### 1-4. パフォーマンスデータの確認
            - `all-reports/dynamic-eval-*/performance-*.json` を読み込む
            - Lighthouse スコア（LCP, FID, CLS）を比較
            - パフォーマンスのボトルネックを特定

            ---

            ## 🔍 Phase 2: 機能の差異分析（超重要 🆕）

            ### Step 1: 機能リストの作成

            各ブランチについて、実装された機能を列挙：

            ```markdown
            #### Branch A (task-xxx)
            実装機能:
            - ✅ 機能1: [詳細]
            - ✅ 機能2: [詳細]
            - ❌ 機能3: 未実装

            #### Branch B (task-yyy)
            実装機能:
            - ✅ 機能1: [詳細、Aとの違い]
            - ❌ 機能2: 未実装
            - ✅ 機能3: [詳細]
            ```

            ### Step 2: 差異比較表の作成

            | 機能 | Branch A | Branch B | Branch C | 評価 |
            |------|----------|----------|----------|------|
            | 機能1 | ✅ 実装方法A | ✅ 実装方法B | ❌ 未実装 | 🥇 A の方が優れている理由 |
            | 機能2 | ❌ 未実装 | ✅ 実装 | ✅ 実装 | 🚨 A に追加すべき |
            | 機能3 | ✅ 過剰実装 | ❌ 未実装 | ❌ 未実装 | ⚠️ A は削除を検討 |

            ### Step 3: 改善提案

            **追加すべき機能**:
            - Branch A に機能2を追加すべき理由: [具体的な根拠]
            - Branch B に機能4を追加すると良い理由: [具体的な根拠]

            **削除すべき過剰な機能**:
            - Branch A の機能3は要件外で複雑性を増している → 削除推奨

            **実装方法の改善**:
            - Branch B の機能1の実装は Branch A の方が優れている理由: [コード引用]

            ---

            ## 🔍 Phase 3: Web検索による最新事例の調査（可能な場合のみ 🌐）

            **注意**: WebSearch ツールが使えない場合は、このフェーズはスキップしてください。

            変更内容を分析した上で、関連する最新のベストプラクティスを検索：

            1. **技術スタックの最新動向**
               - 例: "React 19 best practices 2025"
               - 例: "Three.js performance optimization 2025"

            2. **デザイントレンド**（UI/UX変更がある場合）
               - 例: "Web design trends 2025"
               - 例: "3D web experience UI patterns"

            3. **具体的な実装パターン**
               - 例: "[使用ライブラリ] best practices GitHub"

            検索結果を評価に反映：
            - 最新トレンドとの比較
            - ベストプラクティスの適用度
            - 具体的な改善提案（検索で見つけた事例を引用）

            ---

            ## 📏 Phase 4: 評価軸の決定（固定5軸 + AI 🤖）

            ### ✅ 必須評価軸（必ず評価）

            以下の5つの軸は**必ず評価**してください：

            **1. コード品質** (20点満点)
            - 可読性、保守性、設計パターン
            - 具体的なコードを引用して評価
            - 技術的負債の有無

            **2. パフォーマンス** (20点満点)
            - ビルド時間（`summary.json` の `build_duration`）
            - Lighthouse スコア（`performance-*.json`）
            - バンドルサイズ（`summary.json` の `bundle_size_kb`）
            - 具体的な数値で比較

            **3. デザイン・UI/UX** (20点満点) 🆕
            - スクリーンショットで視覚的品質を確認
            - レイアウト、配色、タイポグラフィ
            - レスポンシブ対応（デスクトップ・モバイル）
            - **新規機能でもデザイン評価は必須**
            - アクセシビリティ（コントラスト、フォントサイズ）

            **4. テスト品質** (20点満点)
            - テストカバレッジ、成功率（`summary.json`）
            - テストコードの充実度
            - エッジケースの処理

            **5. 実装の完全性** (20点満点) 🆕
            - Phase 2 で作成した機能比較表を元に評価
            - 必要な機能が実装されているか
            - 過剰な機能がないか
            - 実装の一貫性

            ### 🤖 AI が考える追加評価軸

            **プロジェクトの特性を分析し**、以下のような独自軸を**自由に追加**してください：

            - セキュリティ（認証、暗号化、入力検証）
            - 国際化対応（i18n, l10n）
            - エラーハンドリング
            - ドキュメント品質
            - 拡張性・スケーラビリティ
            - **[その他、このプロジェクト固有の重要な観点]**

            各追加軸には配点（10点満点推奨）と評価理由を明記してください。

            ---

            ## 🔬 Phase 5: 詳細評価

            各ブランチについて、Phase 4 で決定した評価軸で詳細に分析してください。

            **重要**:
            - 必ず**具体的なコードを引用**
            - スクリーンショットを参照
            - Lighthouse スコアなどの数値を明示
            - 主観的な評価には根拠を示す

            ---

            ## 📄 Phase 6: HTML レポート生成

            `comparison/ai-analysis.html` ファイルを作成してください。

            **必須セクション**:

            1. **Executive Summary** - 評価対象、推奨ブランチ、推奨理由
            2. **総合ランキング** - 1位〜順位付け（スコア表示）
            3. **機能の差異分析** - Phase 2 の比較表と改善提案
            4. **評価軸の説明** - 必須5軸 + AI追加軸
            5. **デザイン・UI/UX評価** - スクリーンショット参照
            6. **詳細分析** - 各ブランチの評価（コード引用）
            7. **Web検索結果**（可能な場合） - 最新事例との比較
            8. **推奨事項** - 今すぐ実施、次のステップ、長期的改善
            9. **結論** - 最終推奨とマージ前の対応

            **HTMLスタイル**:
            - レスポンシブデザイン
            - グラデーション、シャドウ、ホバーエフェクト
            - スクリーンショット表示エリア
            - スコアバー（視覚的に表示）
            - コードハイライト
            - 色分け（良い=緑、警告=オレンジ、悪い=赤）

            ---

            ## ⚠️ 重要な指示（必読）

            1. **ブランチ全体を評価**: コミット単位ではなく、最終成果物として評価
            2. **固定5軸は必須**: コード品質、パフォーマンス、デザイン、テスト、実装完全性
            3. **機能の差異分析は必須**: 各ブランチの機能を列挙し、追加・削除提案
            4. **デザイン評価は必須**: 新規機能でもスクリーンショットを確認
            5. **具体的な根拠を示す**: コード引用、数値、スクショ参照
            6. **HTML 形式で出力**: Markdown ではなく HTML
            7. **プロジェクト固有の分析**: ChocoDrop に合った評価
            8. **Web検索はオプション**: 使えない場合はスキップ

            ---

            ## 🎯 期待される成果物

            - `comparison/ai-analysis.html` - リッチな HTML レポート
            - 各ブランチの総合スコア（100点満点）
            - 明確な推奨ブランチと理由
            - 具体的な改善提案（機能追加・削除・実装方法の改善）
            - ビジュアル評価（スクリーンショット参照）
            - パフォーマンス評価（Lighthouse スコア）
            - 最終的なマージ判断と次のアクション

            頑張ってください！🚀
          claude_code_oauth_token: ${{ secrets.CLAUDE_ACCESS_TOKEN }}

      - name: Upload comparison report
        uses: actions/upload-artifact@v4
        with:
          name: comparison-report
          path: comparison/
          retention-days: 90

      - name: Comment on workflow
        run: |
          echo "📊 Comparison report generated successfully"
          echo "🤖 AI analysis included"
          echo "Download artifacts to view detailed results"
