name: Worktree Parallel Testing
run-name: "${{ github.event.inputs.session_id || 'manual-session' }} · ${{ github.event.inputs.agent_name || github.actor }} · #${{ github.run_number }}"

on:
  workflow_dispatch:
    inputs:
      session_id:
        description: 'Stable session identifier (e.g., task-abc123)'
        required: true
        type: string
      agent_name:
        description: 'Originating agent or human operator'
        required: true
        type: string
      prompt_seed:
        description: 'Normalized task description (<=512 chars)'
        required: false
        type: string
      branches:
        description: 'Comma-separated branch names to test (e.g., task/feature-a,task/feature-b,task/feature-c)'
        required: true
        type: string
      run_build:
        description: 'Run build for each branch'
        required: false
        type: boolean
        default: true
      run_tests:
        description: 'Run tests for each branch'
        required: false
        type: boolean
        default: true
      compare_results:
        description: 'Generate comparison report'
        required: false
        type: boolean
        default: true

permissions:
  contents: read
  pull-requests: write
  issues: write
  id-token: write

env:
  SESSION_ID: ${{ github.event.inputs.session_id }}
  AGENT_NAME: ${{ github.event.inputs.agent_name }}
  PROMPT_SEED: ${{ github.event.inputs.prompt_seed }}

jobs:
  prepare:
    name: Prepare branch matrix
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      branch_count: ${{ steps.set-matrix.outputs.count }}

    steps:
      - name: Validate session metadata
        run: |
          if [[ -z "${{ github.event.inputs.session_id }}" ]]; then
            echo "::error::session_id input is required."
            exit 1
          fi
          if [[ -z "${{ github.event.inputs.agent_name }}" ]]; then
            echo "::error::agent_name input is required."
            exit 1
          fi
          if ! [[ "${{ github.event.inputs.session_id }}" =~ ^[a-zA-Z0-9._-]+$ ]]; then
            echo "::error::session_id must be alphanumeric plus ._-"
            exit 1
          fi
          echo "📌 Session: ${{ github.event.inputs.session_id }}"
          echo "🤖 Agent: ${{ github.event.inputs.agent_name }}"
          if [[ -n "${{ github.event.inputs.prompt_seed }}" ]]; then
            echo "📝 Prompt seed: ${{ github.event.inputs.prompt_seed }}"
          fi

      - name: Parse branches
        id: set-matrix
        run: |
          BRANCHES="${{ github.event.inputs.branches }}"
          # カンマ区切りを配列に変換
          IFS=',' read -ra BRANCH_ARRAY <<< "$BRANCHES"

          # スペースをトリム
          CLEANED_BRANCHES=()
          for branch in "${BRANCH_ARRAY[@]}"; do
            trimmed=$(echo "$branch" | xargs)
            CLEANED_BRANCHES+=("\"$trimmed\"")
          done

          # JSON配列を作成
          BRANCH_JSON="[${CLEANED_BRANCHES[*]}]"
          BRANCH_JSON="${BRANCH_JSON// /,}"

          echo "matrix=$BRANCH_JSON" >> $GITHUB_OUTPUT
          echo "count=${#BRANCH_ARRAY[@]}" >> $GITHUB_OUTPUT

          echo "🌳 Testing ${#BRANCH_ARRAY[@]} branches:"
          for branch in "${BRANCH_ARRAY[@]}"; do
            echo "  - $(echo $branch | xargs)"
          done

  # 各ブランチを並行実行
  test-branch:
    name: Test ${{ matrix.branch }}
    runs-on: ubuntu-latest
    needs: prepare
    env:
      SESSION_ID: ${{ github.event.inputs.session_id }}
      AGENT_NAME: ${{ github.event.inputs.agent_name }}
      PROMPT_SEED: ${{ github.event.inputs.prompt_seed }}
    strategy:
      fail-fast: false
      matrix:
        branch: ${{ fromJson(needs.prepare.outputs.matrix) }}
        node: [20]

    steps:
      - name: Checkout branch ${{ matrix.branch }}
        uses: actions/checkout@v5
        with:
          ref: ${{ matrix.branch }}
          fetch-depth: 0

      - name: Setup Node.js ${{ matrix.node }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node }}
          cache: 'npm'

      - name: Get branch info
        id: branch_info
        run: |
          BRANCH_NAME="${{ matrix.branch }}"
          COMMIT_SHA=$(git rev-parse --short HEAD)
          COMMIT_MSG=$(git log -1 --pretty=%s)
          BRANCH_SAFE=$(echo "$BRANCH_NAME" | sed 's/[^a-zA-Z0-9]/-/g')

          echo "branch_safe=$BRANCH_SAFE" >> $GITHUB_OUTPUT
          echo "commit_sha=$COMMIT_SHA" >> $GITHUB_OUTPUT
          echo "commit_msg=$COMMIT_MSG" >> $GITHUB_OUTPUT

          echo "📋 Branch: $BRANCH_NAME"
          echo "📝 Commit: $COMMIT_SHA - $COMMIT_MSG"

      - name: Install dependencies
        run: |
          echo "📦 Installing dependencies for ${{ matrix.branch }}..."
          npm ci

      - name: Run linting
        id: lint
        run: |
          echo "🔍 Running linting..."
          npm run lint > lint-output.txt 2>&1 || true
          cat lint-output.txt

          if grep -q "error" lint-output.txt; then
            echo "status=failed" >> $GITHUB_OUTPUT
          else
            echo "status=passed" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Run build
        id: build
        if: github.event.inputs.run_build == 'true'
        run: |
          echo "🔨 Building ${{ matrix.branch }}..."
          START_TIME=$(date +%s)

          npm run build > build-output.txt 2>&1 || BUILD_FAILED=1

          END_TIME=$(date +%s)
          BUILD_TIME=$((END_TIME - START_TIME))

          echo "duration=$BUILD_TIME" >> $GITHUB_OUTPUT

          if [ "$BUILD_FAILED" == "1" ]; then
            echo "status=failed" >> $GITHUB_OUTPUT
            cat build-output.txt
            exit 1
          else
            echo "status=passed" >> $GITHUB_OUTPUT
            echo "✅ Build completed in ${BUILD_TIME}s"
          fi
        continue-on-error: true

      - name: Run tests
        id: test
        if: github.event.inputs.run_tests == 'true'
        run: |
          echo "🧪 Running tests for ${{ matrix.branch }}..."
          START_TIME=$(date +%s)

          npm test > test-output.txt 2>&1 || TEST_FAILED=1

          END_TIME=$(date +%s)
          TEST_TIME=$((END_TIME - START_TIME))

          echo "duration=$TEST_TIME" >> $GITHUB_OUTPUT

          if [ "$TEST_FAILED" == "1" ]; then
            echo "status=failed" >> $GITHUB_OUTPUT
            cat test-output.txt
          else
            echo "status=passed" >> $GITHUB_OUTPUT
            echo "✅ Tests completed in ${TEST_TIME}s"
          fi

          # テスト結果をカウント
          PASS_COUNT=$(grep -c "# pass" test-output.txt || echo "0")
          FAIL_COUNT=$(grep -c "# fail" test-output.txt || echo "0")

          echo "pass_count=$PASS_COUNT" >> $GITHUB_OUTPUT
          echo "fail_count=$FAIL_COUNT" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Start dev server for dynamic evaluation
        id: dev_server
        run: |
          echo "🚀 Starting dev server for visual and performance evaluation..."
          npm run dev &
          DEV_PID=$!
          echo "pid=$DEV_PID" >> $GITHUB_OUTPUT

          # Wait for server to be ready
          npx wait-on http://localhost:8000 --timeout 60000 || echo "Server timeout"
        continue-on-error: true

      - name: Install Playwright
        run: |
          npx playwright install --with-deps chromium

      - name: Take screenshots
        id: screenshots
        run: |
          mkdir -p screenshots

          echo "📸 Taking screenshots of examples..."

          # ChocoDrop examples
          EXAMPLES=("basic" "lofi-room" "lofi-city" "music-garden" "space" "toy-city" "wabi-sabi")

          for example in "${EXAMPLES[@]}"; do
            # Desktop
            npx playwright screenshot \
              "http://localhost:8000/examples/$example/" \
              "screenshots/$example-desktop.png" \
              --viewport-size=1920,1080 \
              --timeout=30000 || echo "Screenshot failed: $example desktop"

            # Mobile
            npx playwright screenshot \
              "http://localhost:8000/examples/$example/" \
              "screenshots/$example-mobile.png" \
              --viewport-size=375,667 \
              --timeout=30000 || echo "Screenshot failed: $example mobile"
          done

          echo "status=completed" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Run Lighthouse performance audit
        id: lighthouse
        run: |
          echo "⚡ Running Lighthouse performance audit..."
          npm install -g lighthouse

          # Test key examples
          KEY_EXAMPLES=("basic" "lofi-room" "music-garden")

          for example in "${KEY_EXAMPLES[@]}"; do
            lighthouse "http://localhost:8000/examples/$example/" \
              --output json \
              --output-path "performance-$example.json" \
              --chrome-flags="--headless" \
              --only-categories=performance \
              --preset=desktop \
              --quiet || echo "Lighthouse failed: $example"
          done

          echo "status=completed" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Stop dev server
        if: always()
        run: |
          if [ -n "${{ steps.dev_server.outputs.pid }}" ]; then
            kill ${{ steps.dev_server.outputs.pid }} || true
          fi
          # Kill any remaining node processes
          pkill -f "npm run dev" || true

      - name: Generate branch report
        run: |
          REPORT_DIR="reports/${{ steps.branch_info.outputs.branch_safe }}"
          mkdir -p "$REPORT_DIR"

          # Get branch statistics
          TOTAL_COMMITS=$(git rev-list --count HEAD)
          TOTAL_FILES=$(git ls-tree -r --name-only HEAD | wc -l | xargs)

          # Get bundle size if build succeeded
          BUNDLE_SIZE="0"
          if [ "${{ steps.build.outputs.status }}" == "passed" ] && [ -d "dist" ]; then
            BUNDLE_SIZE=$(du -sk dist | cut -f1)
          fi

          cat > "$REPORT_DIR/summary.json" << EOF
          {
            "branch": "${{ matrix.branch }}",
            "commit": "${{ steps.branch_info.outputs.commit_sha }}",
            "commit_message": "${{ steps.branch_info.outputs.commit_msg }}",
            "total_commits": "$TOTAL_COMMITS",
            "total_files": "$TOTAL_FILES",
            "bundle_size_kb": "$BUNDLE_SIZE",
            "node_version": "${{ matrix.node }}",
            "lint_status": "${{ steps.lint.outputs.status }}",
            "build_status": "${{ steps.build.outputs.status }}",
            "build_duration": "${{ steps.build.outputs.duration }}",
            "test_status": "${{ steps.test.outputs.status }}",
            "test_duration": "${{ steps.test.outputs.duration }}",
            "test_pass": "${{ steps.test.outputs.pass_count }}",
            "test_fail": "${{ steps.test.outputs.fail_count }}",
            "screenshots_status": "${{ steps.screenshots.outputs.status }}",
            "lighthouse_status": "${{ steps.lighthouse.outputs.status }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "evaluation_note": "This represents the final state of the branch (all commits, all files), not individual commits"
          }
          EOF

          # Markdown レポート生成
          cat > "$REPORT_DIR/report.md" << EOF
          # Test Report: ${{ matrix.branch }}

          ## Branch Information
          - **Branch**: \`${{ matrix.branch }}\`
          - **Commit**: \`${{ steps.branch_info.outputs.commit_sha }}\`
          - **Message**: ${{ steps.branch_info.outputs.commit_msg }}
          - **Node.js**: ${{ matrix.node }}

          ## Results

          | Check | Status | Duration |
          |-------|--------|----------|
          | Lint | ${{ steps.lint.outputs.status == 'passed' && '✅ Passed' || '❌ Failed' }} | - |
          | Build | ${{ steps.build.outputs.status == 'passed' && '✅ Passed' || steps.build.outputs.status == 'failed' && '❌ Failed' || '⏭️ Skipped' }} | ${{ steps.build.outputs.duration }}s |
          | Tests | ${{ steps.test.outputs.status == 'passed' && '✅ Passed' || steps.test.outputs.status == 'failed' && '❌ Failed' || '⏭️ Skipped' }} | ${{ steps.test.outputs.duration }}s |

          ### Test Details
          - **Passed**: ${{ steps.test.outputs.pass_count }}
          - **Failed**: ${{ steps.test.outputs.fail_count }}
          EOF

          cat "$REPORT_DIR/report.md"

      - name: Capture session metadata
        run: |
          node - <<'NODE'
          const fs = require('fs');
          const path = require('path');

          const payload = {
            session_id: process.env.SESSION_ID,
            agent: process.env.AGENT_NAME,
            branch: process.env.BRANCH_NAME,
            branch_safe: process.env.BRANCH_SAFE,
            run_id: process.env.RUN_ID,
            run_url: process.env.RUN_URL,
            lint_status: process.env.LINT_STATUS || 'skipped',
            build_status: process.env.BUILD_STATUS || 'skipped',
            build_duration: Number(process.env.BUILD_DURATION || 0),
            test_status: process.env.TEST_STATUS || 'skipped',
            test_duration: Number(process.env.TEST_DURATION || 0),
            test_pass: Number(process.env.TEST_PASS || 0),
            test_fail: Number(process.env.TEST_FAIL || 0),
            prompt_seed: process.env.PROMPT_SEED || '',
            timestamp: new Date().toISOString()
          };

          const metaDir = path.join('session-metadata', payload.session_id);
          fs.mkdirSync(metaDir, { recursive: true });
          const target = path.join(metaDir, `${payload.branch_safe}.json`);
          fs.writeFileSync(target, JSON.stringify(payload, null, 2));
          console.log(`Saved session metadata to ${target}`);
          NODE
        env:
          BRANCH_NAME: ${{ matrix.branch }}
          BRANCH_SAFE: ${{ steps.branch_info.outputs.branch_safe }}
          RUN_ID: ${{ github.run_id }}
          RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          LINT_STATUS: ${{ steps.lint.outputs.status || 'skipped' }}
          BUILD_STATUS: ${{ steps.build.outputs.status || 'skipped' }}
          BUILD_DURATION: ${{ steps.build.outputs.duration || '0' }}
          TEST_STATUS: ${{ steps.test.outputs.status || 'skipped' }}
          TEST_DURATION: ${{ steps.test.outputs.duration || '0' }}
          TEST_PASS: ${{ steps.test.outputs.pass_count || '0' }}
          TEST_FAIL: ${{ steps.test.outputs.fail_count || '0' }}

      - name: Upload branch artifacts
        uses: actions/upload-artifact@v4
        with:
          name: session-${{ github.event.inputs.session_id }}-report-${{ steps.branch_info.outputs.branch_safe }}
          path: reports/
          retention-days: 30

      - name: Upload dynamic evaluation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: session-${{ github.event.inputs.session_id }}-dynamic-${{ steps.branch_info.outputs.branch_safe }}
          path: |
            screenshots/
            performance-*.json
          retention-days: 30
        continue-on-error: true

      - name: Upload session metadata artifact
        uses: actions/upload-artifact@v4
        with:
          name: session-${{ github.event.inputs.session_id }}-metadata
          path: session-metadata/
          retention-days: 30
        continue-on-error: true

      - name: Append session summary
        if: always()
        run: |
          {
            echo "### Session $SESSION_ID · Branch \`${{ matrix.branch }}\`"
            echo ""
            echo "- Agent: $AGENT_NAME"
            echo "- Run: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            echo "- Lint: ${{ steps.lint.outputs.status || 'skipped' }}"
            echo "- Build: ${{ steps.build.outputs.status || 'skipped' }} (duration: ${{ steps.build.outputs.duration || '0' }}s)"
            echo "- Tests: ${{ steps.test.outputs.status || 'skipped' }} (duration: ${{ steps.test.outputs.duration || '0' }}s)"
            echo "- Screenshots: ${{ steps.screenshots.outputs.status || 'n/a' }} | Lighthouse: ${{ steps.lighthouse.outputs.status || 'n/a' }}"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Update session issue
        if: always()
        uses: actions/github-script@v7
        env:
          SESSION_ID: ${{ github.event.inputs.session_id }}
          AGENT_NAME: ${{ github.event.inputs.agent_name }}
          BRANCH: ${{ matrix.branch }}
          RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          LINT_STATUS: ${{ steps.lint.outputs.status || 'skipped' }}
          BUILD_STATUS: ${{ steps.build.outputs.status || 'skipped' }}
          TEST_STATUS: ${{ steps.test.outputs.status || 'skipped' }}
          BUILD_DURATION: ${{ steps.build.outputs.duration || '0' }}
          TEST_DURATION: ${{ steps.test.outputs.duration || '0' }}
          TEST_PASS: ${{ steps.test.outputs.pass_count || '0' }}
          TEST_FAIL: ${{ steps.test.outputs.fail_count || '0' }}
        with:
          script: |
            const sessionId = process.env.SESSION_ID;
            const labelName = `session/${sessionId}`;
            const { owner, repo } = context.repo;

            async function ensureLabel() {
              try {
                await github.rest.issues.getLabel({ owner, repo, name: labelName });
              } catch (error) {
                if (error.status === 404) {
                  await github.rest.issues.createLabel({
                    owner,
                    repo,
                    name: labelName,
                    color: 'BFDADC',
                    description: 'Workflow session tracker'
                  });
                } else {
                  throw error;
                }
              }
            }

            await ensureLabel();

            const issues = await github.paginate(github.rest.issues.listForRepo, {
              owner,
              repo,
              labels: labelName,
              state: 'open',
              per_page: 100
            });

            let issue = issues[0];
            if (!issue) {
              const created = await github.rest.issues.create({
                owner,
                repo,
                title: `Session ${sessionId} tracking`,
                body: `このIssueはセッション **${sessionId}** のワークフロー実行を集約します。`,
                labels: [labelName]
              });
              issue = created.data;
            }

            const commentLines = [
              `### Run #${context.runNumber} · ${process.env.AGENT_NAME}`,
              `- Branch: \`${process.env.BRANCH}\``,
              `- Workflow: [Run link](${process.env.RUN_URL})`,
              `- Lint: ${process.env.LINT_STATUS}`,
              `- Build: ${process.env.BUILD_STATUS} (${process.env.BUILD_DURATION}s)`,
              `- Tests: ${process.env.TEST_STATUS} (${process.env.TEST_DURATION}s, pass=${process.env.TEST_PASS}, fail=${process.env.TEST_FAIL})`,
              '- Artifacts: レポート・スクリーンショット・メタデータを参照してください。'
            ];

            await github.rest.issues.createComment({
              owner,
              repo,
              issue_number: issue.number,
              body: commentLines.join('\n')
            });

  # 全ブランチの結果を比較
  compare:
    name: Compare results
    runs-on: ubuntu-latest
    needs: [prepare, test-branch]
    if: github.event.inputs.compare_results == 'true'
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Download all reports
        uses: actions/download-artifact@v4
        with:
          path: all-reports

      - name: Generate comparison report
        run: |
          mkdir -p comparison

          SESSION_LINE="## Session: ${{ github.event.inputs.session_id }}"
          cat > comparison/README.md << EOF
          # Worktree Parallel Testing - Comparison Report

          $SESSION_LINE
          - Agent: ${{ github.event.inputs.agent_name }}
          - Prompt seed: ${{ github.event.inputs.prompt_seed || 'n/a' }}

          ## Overview
          This report compares the results of parallel testing across multiple branches.

          ## Tested Branches
          EOF

          echo "Testing ${{ needs.prepare.outputs.branch_count }} branches" >> comparison/README.md
          echo "" >> comparison/README.md

          cat >> comparison/README.md << 'EOF'
          ## Comparison Table

          | Branch | Commit | Lint | Build | Build Time | Tests | Test Time | Pass | Fail |
          |--------|--------|------|-------|------------|-------|-----------|------|------|
          EOF

          find all-reports -name summary.json | while read -r JSON_FILE; do
            BRANCH=$(jq -r '.branch' "$JSON_FILE")
            COMMIT=$(jq -r '.commit' "$JSON_FILE")
            LINT=$(jq -r '.lint_status' "$JSON_FILE")
            BUILD=$(jq -r '.build_status' "$JSON_FILE")
            BUILD_DUR=$(jq -r '.build_duration // 0' "$JSON_FILE")
            TEST=$(jq -r '.test_status' "$JSON_FILE")
            TEST_DUR=$(jq -r '.test_duration // 0' "$JSON_FILE")
            PASS=$(jq -r '.test_pass // 0' "$JSON_FILE")
            FAIL=$(jq -r '.test_fail // 0' "$JSON_FILE")

            if [ -z "$BRANCH" ] || [ "$BRANCH" = "null" ]; then
              continue
            fi

            if [ "$LINT" = "passed" ]; then LINT_ICON="✅"; else LINT_ICON="❌"; fi
            if [ "$BUILD" = "passed" ]; then BUILD_ICON="✅"; elif [ "$BUILD" = "failed" ]; then BUILD_ICON="❌"; else BUILD_ICON="⏭️"; fi
            if [ "$TEST" = "passed" ]; then TEST_ICON="✅"; elif [ "$TEST" = "failed" ]; then TEST_ICON="❌"; else TEST_ICON="⏭️"; fi

            echo "| $BRANCH | \`$COMMIT\` | $LINT_ICON | $BUILD_ICON | ${BUILD_DUR}s | $TEST_ICON | ${TEST_DUR}s | $PASS | $FAIL |" >> comparison/README.md
          done

          cat >> comparison/README.md << 'EOF'

          ## Recommendations

          Based on the test results:
          1. ✅ Branches with all tests passing are ready for merge
          2. ⚠️  Branches with build failures need attention
          3. ❌ Branches with test failures require fixes

          ## Next Steps
          1. Review failed branches
          2. Fix any issues
          3. Choose the best implementation
          4. Merge to main branch
          EOF

          cat comparison/README.md

      - name: 🤖 AI Code Comparison with Claude
        uses: anthropics/claude-code-action@beta
        with:
          mode: agent
          direct_prompt: |
            # 🚀 複数ブランチの最終成果物比較評価タスク（改善版 v2.0）

            あなたは経験豊富なシニアエンジニアです。複数のブランチを**最終成果物**として深く分析し、最良の選択を提案してください。

            ## 🌏 言語（最重要 ⚠️⚠️⚠️）

            **❌ 絶対に英語でレポートを書かないでください！**
            **✅ 全てのテキストを日本語で書いてください！**

            - セクション見出し → 日本語
            - 説明文 → 日本語
            - 評価コメント → 日本語
            - 推奨事項 → 日本語
            - コード引用のみ英語でOK

            ---

            ## 🎯 重要な前提条件

            **各ブランチは並列実装されており、途中のコミットは異なります。**
            **❌ コミット単位で比較しないでください。**
            **✅ 各ブランチの最終状態（全ファイル・全機能）を評価してください。**

            ---

            ## 📋 タスク概要

            `all-reports/` ディレクトリに以下のデータが保存されています：
            - テスト結果（`summary.json`）
            - スクリーンショット（`screenshots/`）
            - パフォーマンスデータ（`performance-*.json`）

            これらを**全て確認**し、総合的な比較評価を行ってください。

            ---

            ## 🔍 Phase 1: ブランチの最終成果物を把握

            ### 1-1. テスト結果の読み込み
            - `all-reports/report-*/reports/*/summary.json` を全て読み込む
            - ビルド時間、テスト成功率、バンドルサイズを把握
            - **注意**: `evaluation_note` に「最終状態を評価」と明記されています

            ### 1-2. ブランチ全体のコードを読み込む（必須 🚨）

            **各ブランチについて**、以下を実施してください：

            a. **プロジェクト構成の把握**
            ```bash
            # ブランチのファイル一覧を取得
            git ls-tree -r --name-only <branch>
            ```

            b. **主要ディレクトリを全て確認**
            - `src/` - ソースコード全体を読む
            - `examples/` - 実装例・デモを確認
            - `test/` - テストコードを確認
            - `package.json` - 依存関係を確認
            - その他設定ファイル

            c. **実装内容の全体像を把握**
            - どんな機能が実装されているか（全て列挙）
            - どんなファイル構成になっているか
            - どんな設計パターンを使っているか
            - どんな外部ライブラリを使っているか

            **❌ やってはいけないこと**:
            - コミット単位で比較する
            - 「変更ファイル」だけを見る
            - `git diff` だけを見る

            **✅ やるべきこと**:
            - ブランチ全体のファイルを読む
            - 実装されている機能を全て列挙
            - 最終成果物として評価

            ### 1-3. ビジュアル評価データの確認
            - `all-reports/dynamic-eval-*/screenshots/` のスクリーンショットを確認
            - デスクトップ版とモバイル版を比較
            - デザインの一貫性、レイアウト、配色を評価

            ### 1-4. パフォーマンスデータの確認
            - `all-reports/dynamic-eval-*/performance-*.json` を読み込む
            - Lighthouse スコア（LCP, FID, CLS）を比較
            - パフォーマンスのボトルネックを特定

            ---

            ## 🔍 Phase 2: 機能の差異分析（超重要 🆕）

            ### Step 1: 機能リストの作成

            各ブランチについて、実装された機能を列挙：

            ```markdown
            #### Branch A (task-xxx)
            実装機能:
            - ✅ 機能1: [詳細]
            - ✅ 機能2: [詳細]
            - ❌ 機能3: 未実装

            #### Branch B (task-yyy)
            実装機能:
            - ✅ 機能1: [詳細、Aとの違い]
            - ❌ 機能2: 未実装
            - ✅ 機能3: [詳細]
            ```

            ### Step 2: 差異比較表の作成

            | 機能 | Branch A | Branch B | Branch C | 評価 |
            |------|----------|----------|----------|------|
            | 機能1 | ✅ 実装方法A | ✅ 実装方法B | ❌ 未実装 | 🥇 A の方が優れている理由 |
            | 機能2 | ❌ 未実装 | ✅ 実装 | ✅ 実装 | 🚨 A に追加すべき |
            | 機能3 | ✅ 過剰実装 | ❌ 未実装 | ❌ 未実装 | ⚠️ A は削除を検討 |

            ### Step 3: 改善提案

            **追加すべき機能**:
            - Branch A に機能2を追加すべき理由: [具体的な根拠]
            - Branch B に機能4を追加すると良い理由: [具体的な根拠]

            **削除すべき過剰な機能**:
            - Branch A の機能3は要件外で複雑性を増している → 削除推奨

            **実装方法の改善**:
            - Branch B の機能1の実装は Branch A の方が優れている理由: [コード引用]

            ---

            ## 🔍 Phase 3: Web検索による最新事例の調査（可能な場合のみ 🌐）

            **注意**: WebSearch ツールが使えない場合は、このフェーズはスキップしてください。

            変更内容を分析した上で、関連する最新のベストプラクティスを検索：

            1. **技術スタックの最新動向**
               - 例: "React 19 best practices 2025"
               - 例: "Three.js performance optimization 2025"

            2. **デザイントレンド**（UI/UX変更がある場合）
               - 例: "Web design trends 2025"
               - 例: "3D web experience UI patterns"

            3. **具体的な実装パターン**
               - 例: "[使用ライブラリ] best practices GitHub"

            検索結果を評価に反映：
            - 最新トレンドとの比較
            - ベストプラクティスの適用度
            - 具体的な改善提案（検索で見つけた事例を引用）

            ---

            ## 📏 Phase 4: 評価軸の決定（二層構造 + AI追加軸 🤖）

            ## 🎯 評価の哲学

            **AIの優秀さは創造力の証であり、プロダクトの優秀さは人間社会への接続の証です。**

            評価は以下の二層で行ってください：
            - **Layer 1**: AIの知的成熟度（どれだけ優秀なAIか）
            - **Layer 2**: プロダクトとしての価値（人間にとってどれだけ価値があるか）

            ---

            ## 🧠 Layer 1: AI知的成熟度（75点満点）

            **AIエージェントとしての「思考・判断の質」を評価します。**

            ### 1. 自己整合性（15点満点）
            - コード全体の論理的一貫性
            - アーキテクチャの統一性
            - 命名規則・スタイルの統一
            - 技術的負債の少なさ
            - **評価**: 具体的なコードを引用して評価

            ### 2. 創造性（15点満点）
            - 独自のアプローチ・創意工夫
            - 既存パターンからの脱却
            - 問題解決の新規性
            - アイデアの独創性
            - **評価**: 他のブランチと比較して何が独自か明記

            ### 3. 技術的品質（20点満点）
            - パフォーマンス（ビルド時間、Lighthouse スコア）
            - バンドルサイズ、メモリ効率
            - コードの可読性・保守性
            - テストカバレッジ、成功率
            - **評価**: `summary.json` と `performance-*.json` の数値を明示

            ### 4. 実装完全性（15点満点）
            - Phase 2 の機能比較表を元に評価
            - 必要な機能の実装度
            - 過剰な機能がないか
            - エッジケースの処理
            - **評価**: 機能リストと照らし合わせて評価

            ### 5. 推論の透明性（10点満点）
            - なぜその実装・アプローチを選んだか
            - コメント・ドキュメントでロジックを説明
            - 思考プロセスが追跡可能
            - コミットメッセージ、設計判断の記録
            - **評価**: README、コメント、コミット履歴を確認

            ---

            ## 💡 Layer 2: プロダクトとしての価値（60点満点）

            **AIが作った成果物が人間社会にどう機能するかを評価します。**

            ### 1. 目的適合性（20点満点）

            **二段階評価**:
            - **仕様適合（60%）**: 指定された機能要件・ユーザーストーリーへの適合度
            - **ビジョン接続（40%）**: プロジェクトの大義・理念への理解と貢献

            **評価項目**:
            - 開発の自己目的化を防ぐ
            - ユーザー価値の連鎖（仕様→実装→価値）
            - 本来の課題解決への貢献度
            - プロジェクトの「なぜ作るのか」への共鳴度
            - **評価**: 元の要求と実装の整合性、プロジェクトの理念への反映度を検証

            ### 2. 体験設計（UX）- 認知的UX（15点満点）

            **操作性・理解・快適性を評価します。**

            - 直感性: 使い方がすぐわかるか
            - 一貫性: 操作パターンが統一されているか
            - 学習コスト: 習得にかかる時間
            - 情緒性（affectiveness）: 使って気持ちいいか
            - **評価**: スクリーンショット、UI構造、ユーザーフローを確認

            ### 3. セキュリティ・データ倫理（10点満点）
            - センシティブデータの適切な扱い（位置、視覚、生体情報等）
            - 不必要なデータ収集をしていないか
            - アクセス権限の適切性
            - プライバシー配慮
            - **評価**: データフロー、権限設定を確認

            ### 4. 持続可能性（10点満点）
            - 長期的なメンテナンス性
            - 拡張性・スケーラビリティ
            - 依存関係の健全性
            - 「未来に残るプロダクト」であるか
            - **評価**: アーキテクチャ、依存ライブラリを確認

            ### 5. 文化的・倫理的配慮 - 社会的UX（5点満点）

            **共感・倫理・多様性を評価します。**

            - 文化的多様性への配慮
            - Inclusivity（包摂性）
            - 偏見の再生産を避けているか
            - 社会的文脈での意味
            - **評価**: UI表現、言語選択、デフォルト設定を確認

            ---

            ## 🤖 AI追加評価軸（プロジェクト依存、最大30点）

            **プロジェクトの本質を深く理解し、1〜3軸を追加してください。**

            **重要な注意事項**:
            - ❌ 下記の例に引っ張られないでください
            - ✅ プロジェクトの性質・目的・技術領域を分析し、本当に重要な軸を判断してください
            - ✅ プロダクトの複雑性に応じて軸の数を調整してください（1〜3軸）

            **例（あくまで参考）**:
            - **XRプロダクト** → 「体験倫理（10点）」没入感のバランス、気持ち悪さの回避
            - **レポート・調査** → 「調査の深さ（10点）」情報源の質、網羅性、洞察の深さ
            - **セキュリティシステム** → 「脆弱性対策（10点）」「緊急対応性（10点）」
            - **小説** → 「文学性（10点）」「読みやすさ（10点）」
            - **防犯ブザー** → 「緊急対応性（10点）」「信頼性（10点）」「物理的耐久性（10点）」
            - **AI推論が重要なプロダクト** → 「推論プロセスの記録（10点）」思考過程の可視化

            **追加軸の条件**:
            - 各軸は10点満点
            - 評価理由を明記
            - プロジェクト固有の価値を測る軸であること
            - 合計30点を超えないこと（ボーナス枠として扱う）

            ---

            ## 🔬 Phase 5: 詳細評価

            各ブランチについて、Phase 4 で決定した評価軸で詳細に分析してください。

            **重要**:
            - 必ず**具体的なコードを引用**
            - スクリーンショットを参照
            - Lighthouse スコアなどの数値を明示
            - 主観的な評価には根拠を示す

            ---

            ## 📄 Phase 6: HTML レポート生成

            ## 🌏 言語（超重要 ⚠️⚠️⚠️）

            **❌ 絶対にやってはいけないこと**:
            - 英語でレポートを作成する

            **✅ 必ずやること**:
            - **全てのテキストを日本語で書く**
            - セクション見出し、説明文、評価コメント、推奨事項など全て日本語
            - コード引用は除く（コードは英語でOK）

            ---

            `comparison/ai-analysis.html` ファイルを作成してください。

            **必須セクション**:

            1. **Executive Summary** - 評価対象、推奨ブランチ、推奨理由、二層スコア表示
            2. **総合ランキング** - 1位〜順位付け（Layer 1、Layer 2、AI追加軸、総合スコア表示）
            3. **機能の差異分析** - Phase 2 の比較表と改善提案
            4. **評価軸の説明** - Layer 1（75点）、Layer 2（60点）、AI追加軸（最大30点）
            5. **デザイン・UI/UX評価** - スクリーンショット参照
            6. **詳細分析** - 各ブランチの評価（コード引用）
            7. **Web検索結果**（可能な場合） - 最新事例との比較
            8. **推奨事項** - 今すぐ実施、次のステップ、長期的改善
            9. **結論** - 最終推奨とマージ前の対応

            ## 🎨 HTMLデザイン（超重要 ⚠️）

            **レポートのデザイン自体がプロダクトのマーケティング・アピールです。**

            ### デザイン方針

            **❌ やってはいけないこと**:
            - 毎回同じ紫のデザインを使う
            - 汎用的な「レポートっぽい」デザイン

            **✅ やるべきこと**:
            - プロダクトの本質を視覚的に表現する
            - プロジェクトの性質・雰囲気に合わせたデザイン
            - 色・レイアウト・フォントをプロダクトに合わせる

            ### プロダクト別デザイン例（参考）

            - **XR/VR**: 未来的、グラデーション（青→紫）、空間的レイアウト、立体感
            - **セキュリティ**: シャープ、信頼感、青系、警告色（赤・黄）、堅実なフォント
            - **小説・文学**: 温かみ、読書的、セリフ体、紙のテクスチャ、落ち着いた色
            - **防犯ブザー**: 緊急性、赤・黄色、警告的、視認性重視、大きなボタン
            - **音楽アプリ**: リズム感、波形、音符、グラデーション、動的な配色
            - **子供向け**: カラフル、丸みのあるフォント、イラスト的、楽しさ

            ### 必須要素

            - レスポンシブデザイン
            - グラデーション、シャドウ、ホバーエフェクト（プロダクトに合わせて）
            - スクリーンショット表示エリア
            - スコアバー（Layer 1 と Layer 2 を分けて表示）
            - コードハイライト
            - 色分け（良い=緑、警告=オレンジ、悪い=赤）
            - **プロダクトの雰囲気を反映したカラーパレット**
            - **プロダクトに合ったフォント選択**

            ---

            ## ⚠️ 重要な指示（必読）

            1. **❗ 必ず日本語で作成**: 全てのテキストを日本語で書く（コード引用を除く）
            2. **ブランチ全体を評価**: コミット単位ではなく、最終成果物として評価
            3. **二層構造は必須**: Layer 1（AI知的成熟度75点）+ Layer 2（プロダクト価値60点）
            4. **AI追加軸は柔軟に**: プロジェクトの本質を見て1〜3軸を追加（最大30点、例に引っ張られない）
            5. **推論の透明性を評価**: README、コメント、コミット履歴で思考プロセスを確認
            6. **目的適合性は二段階**: 仕様適合（60%）+ ビジョン接続（40%）
            7. **機能の差異分析は必須**: 各ブランチの機能を列挙し、追加・削除提案
            8. **デザイン評価は必須**: 新規機能でもスクリーンショットを確認
            9. **具体的な根拠を示す**: コード引用、数値、スクショ参照
            10. **HTML 形式で出力**: Markdown ではなく HTML
            11. **プロダクトに合わせたHTMLデザイン**: 毎回同じ紫じゃなく、プロダクトの雰囲気を反映
            12. **Web検索はオプション**: 使えない場合はスキップ

            ---

            ## 🎯 期待される成果物

            - `comparison/ai-analysis.html` - **プロダクトの雰囲気を反映した**リッチな HTML レポート
            - 各ブランチの総合スコア（Layer 1: 75点 + Layer 2: 60点 + AI追加軸: 最大30点 = 最大165点）
            - **二層構造の評価**: AIの知的成熟度 vs プロダクト価値
            - **推論の透明性評価**: 思考プロセスの記録・説明
            - 明確な推奨ブランチと理由
            - 具体的な改善提案（機能追加・削除・実装方法の改善）
            - ビジュアル評価（スクリーンショット参照）
            - パフォーマンス評価（Lighthouse スコア）
            - 最終的なマージ判断と次のアクション
            - **「どのAIが最も人間と世界を理解していたか」の分析**

            頑張ってください！🚀
          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}

      - name: Upload comparison report
        uses: actions/upload-artifact@v4
        with:
          name: comparison-report
          path: comparison/
          retention-days: 90

      - name: Comment on workflow
        run: |
          echo "📊 Comparison report generated successfully"
          echo "🤖 AI analysis included"
          echo "Download artifacts to view detailed results"
