name: Worktree Parallel Testing

on:
  workflow_dispatch:
    inputs:
      branches:
        description: 'Comma-separated branch names to test (e.g., task/feature-a,task/feature-b,task/feature-c)'
        required: true
        type: string
      run_build:
        description: 'Run build for each branch'
        required: false
        type: boolean
        default: true
      run_tests:
        description: 'Run tests for each branch'
        required: false
        type: boolean
        default: true
      compare_results:
        description: 'Generate comparison report'
        required: false
        type: boolean
        default: true

permissions:
  contents: read
  pull-requests: write
  issues: write
  id-token: write

jobs:
  prepare:
    name: Prepare branch matrix
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      branch_count: ${{ steps.set-matrix.outputs.count }}

    steps:
      - name: Parse branches
        id: set-matrix
        run: |
          BRANCHES="${{ github.event.inputs.branches }}"
          # カンマ区切りを配列に変換
          IFS=',' read -ra BRANCH_ARRAY <<< "$BRANCHES"

          # スペースをトリム
          CLEANED_BRANCHES=()
          for branch in "${BRANCH_ARRAY[@]}"; do
            trimmed=$(echo "$branch" | xargs)
            CLEANED_BRANCHES+=("\"$trimmed\"")
          done

          # JSON配列を作成
          BRANCH_JSON="[${CLEANED_BRANCHES[*]}]"
          BRANCH_JSON="${BRANCH_JSON// /,}"

          echo "matrix=$BRANCH_JSON" >> $GITHUB_OUTPUT
          echo "count=${#BRANCH_ARRAY[@]}" >> $GITHUB_OUTPUT

          echo "🌳 Testing ${#BRANCH_ARRAY[@]} branches:"
          for branch in "${BRANCH_ARRAY[@]}"; do
            echo "  - $(echo $branch | xargs)"
          done

  # 各ブランチを並行実行
  test-branch:
    name: Test ${{ matrix.branch }}
    runs-on: ubuntu-latest
    needs: prepare
    strategy:
      fail-fast: false
      matrix:
        branch: ${{ fromJson(needs.prepare.outputs.matrix) }}
        node: [20]

    steps:
      - name: Checkout branch ${{ matrix.branch }}
        uses: actions/checkout@v4
        with:
          ref: ${{ matrix.branch }}
          fetch-depth: 0

      - name: Setup Node.js ${{ matrix.node }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node }}
          cache: 'npm'

      - name: Get branch info
        id: branch_info
        run: |
          BRANCH_NAME="${{ matrix.branch }}"
          COMMIT_SHA=$(git rev-parse --short HEAD)
          COMMIT_MSG=$(git log -1 --pretty=%s)
          BRANCH_SAFE=$(echo "$BRANCH_NAME" | sed 's/[^a-zA-Z0-9]/-/g')

          echo "branch_safe=$BRANCH_SAFE" >> $GITHUB_OUTPUT
          echo "commit_sha=$COMMIT_SHA" >> $GITHUB_OUTPUT
          echo "commit_msg=$COMMIT_MSG" >> $GITHUB_OUTPUT

          echo "📋 Branch: $BRANCH_NAME"
          echo "📝 Commit: $COMMIT_SHA - $COMMIT_MSG"

      - name: Install dependencies
        run: |
          echo "📦 Installing dependencies for ${{ matrix.branch }}..."
          npm ci

      - name: Run linting
        id: lint
        run: |
          echo "🔍 Running linting..."
          npm run lint > lint-output.txt 2>&1 || true
          cat lint-output.txt

          if grep -q "error" lint-output.txt; then
            echo "status=failed" >> $GITHUB_OUTPUT
          else
            echo "status=passed" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Run build
        id: build
        if: github.event.inputs.run_build == 'true'
        run: |
          echo "🔨 Building ${{ matrix.branch }}..."
          START_TIME=$(date +%s)

          npm run build > build-output.txt 2>&1 || BUILD_FAILED=1

          END_TIME=$(date +%s)
          BUILD_TIME=$((END_TIME - START_TIME))

          echo "duration=$BUILD_TIME" >> $GITHUB_OUTPUT

          if [ "$BUILD_FAILED" == "1" ]; then
            echo "status=failed" >> $GITHUB_OUTPUT
            cat build-output.txt
            exit 1
          else
            echo "status=passed" >> $GITHUB_OUTPUT
            echo "✅ Build completed in ${BUILD_TIME}s"
          fi
        continue-on-error: true

      - name: Run tests
        id: test
        if: github.event.inputs.run_tests == 'true'
        run: |
          echo "🧪 Running tests for ${{ matrix.branch }}..."
          START_TIME=$(date +%s)

          npm test > test-output.txt 2>&1 || TEST_FAILED=1

          END_TIME=$(date +%s)
          TEST_TIME=$((END_TIME - START_TIME))

          echo "duration=$TEST_TIME" >> $GITHUB_OUTPUT

          if [ "$TEST_FAILED" == "1" ]; then
            echo "status=failed" >> $GITHUB_OUTPUT
            cat test-output.txt
          else
            echo "status=passed" >> $GITHUB_OUTPUT
            echo "✅ Tests completed in ${TEST_TIME}s"
          fi

          # テスト結果をカウント
          PASS_COUNT=$(grep -c "# pass" test-output.txt || echo "0")
          FAIL_COUNT=$(grep -c "# fail" test-output.txt || echo "0")

          echo "pass_count=$PASS_COUNT" >> $GITHUB_OUTPUT
          echo "fail_count=$FAIL_COUNT" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Start dev server for dynamic evaluation
        id: dev_server
        run: |
          echo "🚀 Starting dev server for visual and performance evaluation..."
          npm run dev &
          DEV_PID=$!
          echo "pid=$DEV_PID" >> $GITHUB_OUTPUT

          # Wait for server to be ready
          npx wait-on http://localhost:8000 --timeout 60000 || echo "Server timeout"
        continue-on-error: true

      - name: Install Playwright
        run: |
          npx playwright install --with-deps chromium

      - name: Take screenshots
        id: screenshots
        run: |
          mkdir -p screenshots

          echo "📸 Taking screenshots of examples..."

          # ChocoDrop examples
          EXAMPLES=("basic" "lofi-room" "lofi-city" "music-garden" "space" "toy-city" "wabi-sabi")

          for example in "${EXAMPLES[@]}"; do
            # Desktop
            npx playwright screenshot \
              "http://localhost:8000/examples/$example/" \
              "screenshots/$example-desktop.png" \
              --viewport-size=1920,1080 \
              --timeout=30000 || echo "Screenshot failed: $example desktop"

            # Mobile
            npx playwright screenshot \
              "http://localhost:8000/examples/$example/" \
              "screenshots/$example-mobile.png" \
              --viewport-size=375,667 \
              --timeout=30000 || echo "Screenshot failed: $example mobile"
          done

          echo "status=completed" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Run Lighthouse performance audit
        id: lighthouse
        run: |
          echo "⚡ Running Lighthouse performance audit..."
          npm install -g lighthouse

          # Test key examples
          KEY_EXAMPLES=("basic" "lofi-room" "music-garden")

          for example in "${KEY_EXAMPLES[@]}"; do
            lighthouse "http://localhost:8000/examples/$example/" \
              --output json \
              --output-path "performance-$example.json" \
              --chrome-flags="--headless" \
              --only-categories=performance \
              --preset=desktop \
              --quiet || echo "Lighthouse failed: $example"
          done

          echo "status=completed" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Stop dev server
        if: always()
        run: |
          if [ -n "${{ steps.dev_server.outputs.pid }}" ]; then
            kill ${{ steps.dev_server.outputs.pid }} || true
          fi
          # Kill any remaining node processes
          pkill -f "npm run dev" || true

      - name: Generate branch report
        run: |
          REPORT_DIR="reports/${{ steps.branch_info.outputs.branch_safe }}"
          mkdir -p "$REPORT_DIR"

          # Get branch statistics
          TOTAL_COMMITS=$(git rev-list --count HEAD)
          TOTAL_FILES=$(git ls-tree -r --name-only HEAD | wc -l | xargs)

          # Get bundle size if build succeeded
          BUNDLE_SIZE="0"
          if [ "${{ steps.build.outputs.status }}" == "passed" ] && [ -d "dist" ]; then
            BUNDLE_SIZE=$(du -sk dist | cut -f1)
          fi

          cat > "$REPORT_DIR/summary.json" << EOF
          {
            "branch": "${{ matrix.branch }}",
            "commit": "${{ steps.branch_info.outputs.commit_sha }}",
            "commit_message": "${{ steps.branch_info.outputs.commit_msg }}",
            "total_commits": "$TOTAL_COMMITS",
            "total_files": "$TOTAL_FILES",
            "bundle_size_kb": "$BUNDLE_SIZE",
            "node_version": "${{ matrix.node }}",
            "lint_status": "${{ steps.lint.outputs.status }}",
            "build_status": "${{ steps.build.outputs.status }}",
            "build_duration": "${{ steps.build.outputs.duration }}",
            "test_status": "${{ steps.test.outputs.status }}",
            "test_duration": "${{ steps.test.outputs.duration }}",
            "test_pass": "${{ steps.test.outputs.pass_count }}",
            "test_fail": "${{ steps.test.outputs.fail_count }}",
            "screenshots_status": "${{ steps.screenshots.outputs.status }}",
            "lighthouse_status": "${{ steps.lighthouse.outputs.status }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "evaluation_note": "This represents the final state of the branch (all commits, all files), not individual commits"
          }
          EOF

          # Markdown レポート生成
          cat > "$REPORT_DIR/report.md" << EOF
          # Test Report: ${{ matrix.branch }}

          ## Branch Information
          - **Branch**: \`${{ matrix.branch }}\`
          - **Commit**: \`${{ steps.branch_info.outputs.commit_sha }}\`
          - **Message**: ${{ steps.branch_info.outputs.commit_msg }}
          - **Node.js**: ${{ matrix.node }}

          ## Results

          | Check | Status | Duration |
          |-------|--------|----------|
          | Lint | ${{ steps.lint.outputs.status == 'passed' && '✅ Passed' || '❌ Failed' }} | - |
          | Build | ${{ steps.build.outputs.status == 'passed' && '✅ Passed' || steps.build.outputs.status == 'failed' && '❌ Failed' || '⏭️ Skipped' }} | ${{ steps.build.outputs.duration }}s |
          | Tests | ${{ steps.test.outputs.status == 'passed' && '✅ Passed' || steps.test.outputs.status == 'failed' && '❌ Failed' || '⏭️ Skipped' }} | ${{ steps.test.outputs.duration }}s |

          ### Test Details
          - **Passed**: ${{ steps.test.outputs.pass_count }}
          - **Failed**: ${{ steps.test.outputs.fail_count }}
          EOF

          cat "$REPORT_DIR/report.md"

      - name: Upload branch artifacts
        uses: actions/upload-artifact@v4
        with:
          name: report-${{ steps.branch_info.outputs.branch_safe }}
          path: reports/
          retention-days: 30

      - name: Upload dynamic evaluation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dynamic-eval-${{ steps.branch_info.outputs.branch_safe }}
          path: |
            screenshots/
            performance-*.json
          retention-days: 30
        continue-on-error: true

  # 全ブランチの結果を比較
  compare:
    name: Compare results
    runs-on: ubuntu-latest
    needs: [prepare, test-branch]
    if: github.event.inputs.compare_results == 'true'
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all reports
        uses: actions/download-artifact@v4
        with:
          path: all-reports

      - name: Generate comparison report
        run: |
          mkdir -p comparison

          cat > comparison/README.md << 'EOF'
          # Worktree Parallel Testing - Comparison Report

          ## Overview
          This report compares the results of parallel testing across multiple branches.

          ## Tested Branches
          EOF

          echo "Testing ${{ needs.prepare.outputs.branch_count }} branches" >> comparison/README.md
          echo "" >> comparison/README.md

          # 各ブランチのレポートを統合
          cat >> comparison/README.md << 'EOF'
          ## Comparison Table

          | Branch | Commit | Lint | Build | Build Time | Tests | Test Time | Pass | Fail |
          |--------|--------|------|-------|------------|-------|-----------|------|------|
          EOF

          for report_dir in all-reports/report-*/; do
            if [ -f "$report_dir/*/summary.json" ]; then
              JSON_FILE=$(find "$report_dir" -name "summary.json" | head -1)

              BRANCH=$(jq -r '.branch' "$JSON_FILE")
              COMMIT=$(jq -r '.commit' "$JSON_FILE")
              LINT=$(jq -r '.lint_status' "$JSON_FILE")
              BUILD=$(jq -r '.build_status' "$JSON_FILE")
              BUILD_DUR=$(jq -r '.build_duration' "$JSON_FILE")
              TEST=$(jq -r '.test_status' "$JSON_FILE")
              TEST_DUR=$(jq -r '.test_duration' "$JSON_FILE")
              PASS=$(jq -r '.test_pass' "$JSON_FILE")
              FAIL=$(jq -r '.test_fail' "$JSON_FILE")

              LINT_ICON=$([[ "$LINT" == "passed" ]] && echo "✅" || echo "❌")
              BUILD_ICON=$([[ "$BUILD" == "passed" ]] && echo "✅" || [[ "$BUILD" == "failed" ]] && echo "❌" || echo "⏭️")
              TEST_ICON=$([[ "$TEST" == "passed" ]] && echo "✅" || [[ "$TEST" == "failed" ]] && echo "❌" || echo "⏭️")

              echo "| $BRANCH | \`$COMMIT\` | $LINT_ICON | $BUILD_ICON | ${BUILD_DUR}s | $TEST_ICON | ${TEST_DUR}s | $PASS | $FAIL |" >> comparison/README.md
            fi
          done

          cat >> comparison/README.md << 'EOF'

          ## Recommendations

          Based on the test results:
          1. ✅ Branches with all tests passing are ready for merge
          2. ⚠️  Branches with build failures need attention
          3. ❌ Branches with test failures require fixes

          ## Next Steps
          1. Review failed branches
          2. Fix any issues
          3. Choose the best implementation
          4. Merge to main branch
          EOF

          cat comparison/README.md

      - name: 🤖 AI Code Comparison with Claude
        uses: anthropics/claude-code-action@beta
        with:
          mode: agent
          direct_prompt: |
            # 🚀 複数ブランチの最終成果物比較評価タスク（改善版 v2.0）

            あなたは経験豊富なシニアエンジニアです。複数のブランチを**最終成果物**として深く分析し、最良の選択を提案してください。

            ## 🌏 言語（最重要 ⚠️⚠️⚠️）

            **❌ 絶対に英語でレポートを書かないでください！**
            **✅ 全てのテキストを日本語で書いてください！**

            - セクション見出し → 日本語
            - 説明文 → 日本語
            - 評価コメント → 日本語
            - 推奨事項 → 日本語
            - コード引用のみ英語でOK

            ---

            ## 🎯 重要な前提条件

            **各ブランチは並列実装されており、途中のコミットは異なります。**
            **❌ コミット単位で比較しないでください。**
            **✅ 各ブランチの最終状態（全ファイル・全機能）を評価してください。**

            ---

            ## 📋 タスク概要

            `all-reports/` ディレクトリに以下のデータが保存されています：
            - テスト結果（`summary.json`）
            - スクリーンショット（`screenshots/`）
            - パフォーマンスデータ（`performance-*.json`）

            これらを**全て確認**し、総合的な比較評価を行ってください。

            ---

            ## 🔍 Phase 1: ブランチの最終成果物を把握

            ### 1-1. テスト結果の読み込み
            - `all-reports/report-*/reports/*/summary.json` を全て読み込む
            - ビルド時間、テスト成功率、バンドルサイズを把握
            - **注意**: `evaluation_note` に「最終状態を評価」と明記されています

            ### 1-2. ブランチ全体のコードを読み込む（必須 🚨）

            **各ブランチについて**、以下を実施してください：

            a. **プロジェクト構成の把握**
            ```bash
            # ブランチのファイル一覧を取得
            git ls-tree -r --name-only <branch>
            ```

            b. **主要ディレクトリを全て確認**
            - `src/` - ソースコード全体を読む
            - `examples/` - 実装例・デモを確認
            - `test/` - テストコードを確認
            - `package.json` - 依存関係を確認
            - その他設定ファイル

            c. **実装内容の全体像を把握**
            - どんな機能が実装されているか（全て列挙）
            - どんなファイル構成になっているか
            - どんな設計パターンを使っているか
            - どんな外部ライブラリを使っているか

            **❌ やってはいけないこと**:
            - コミット単位で比較する
            - 「変更ファイル」だけを見る
            - `git diff` だけを見る

            **✅ やるべきこと**:
            - ブランチ全体のファイルを読む
            - 実装されている機能を全て列挙
            - 最終成果物として評価

            ### 1-3. ビジュアル評価データの確認
            - `all-reports/dynamic-eval-*/screenshots/` のスクリーンショットを確認
            - デスクトップ版とモバイル版を比較
            - デザインの一貫性、レイアウト、配色を評価

            ### 1-4. パフォーマンスデータの確認
            - `all-reports/dynamic-eval-*/performance-*.json` を読み込む
            - Lighthouse スコア（LCP, FID, CLS）を比較
            - パフォーマンスのボトルネックを特定

            ---

            ## 🔍 Phase 2: 機能の差異分析（超重要 🆕）

            ### Step 1: 機能リストの作成

            各ブランチについて、実装された機能を列挙：

            ```markdown
            #### Branch A (task-xxx)
            実装機能:
            - ✅ 機能1: [詳細]
            - ✅ 機能2: [詳細]
            - ❌ 機能3: 未実装

            #### Branch B (task-yyy)
            実装機能:
            - ✅ 機能1: [詳細、Aとの違い]
            - ❌ 機能2: 未実装
            - ✅ 機能3: [詳細]
            ```

            ### Step 2: 差異比較表の作成

            | 機能 | Branch A | Branch B | Branch C | 評価 |
            |------|----------|----------|----------|------|
            | 機能1 | ✅ 実装方法A | ✅ 実装方法B | ❌ 未実装 | 🥇 A の方が優れている理由 |
            | 機能2 | ❌ 未実装 | ✅ 実装 | ✅ 実装 | 🚨 A に追加すべき |
            | 機能3 | ✅ 過剰実装 | ❌ 未実装 | ❌ 未実装 | ⚠️ A は削除を検討 |

            ### Step 3: 改善提案

            **追加すべき機能**:
            - Branch A に機能2を追加すべき理由: [具体的な根拠]
            - Branch B に機能4を追加すると良い理由: [具体的な根拠]

            **削除すべき過剰な機能**:
            - Branch A の機能3は要件外で複雑性を増している → 削除推奨

            **実装方法の改善**:
            - Branch B の機能1の実装は Branch A の方が優れている理由: [コード引用]

            ---

            ## 🔍 Phase 3: Web検索による最新事例の調査（可能な場合のみ 🌐）

            **注意**: WebSearch ツールが使えない場合は、このフェーズはスキップしてください。

            変更内容を分析した上で、関連する最新のベストプラクティスを検索：

            1. **技術スタックの最新動向**
               - 例: "React 19 best practices 2025"
               - 例: "Three.js performance optimization 2025"

            2. **デザイントレンド**（UI/UX変更がある場合）
               - 例: "Web design trends 2025"
               - 例: "3D web experience UI patterns"

            3. **具体的な実装パターン**
               - 例: "[使用ライブラリ] best practices GitHub"

            検索結果を評価に反映：
            - 最新トレンドとの比較
            - ベストプラクティスの適用度
            - 具体的な改善提案（検索で見つけた事例を引用）

            ---

            ## 📏 Phase 4: 評価軸の決定（二層構造 + AI追加軸 🤖）

            ## 🎯 評価の哲学

            **AIの優秀さは創造力の証であり、プロダクトの優秀さは人間社会への接続の証です。**

            評価は以下の二層で行ってください：
            - **Layer 1**: AIの知的成熟度（どれだけ優秀なAIか）
            - **Layer 2**: プロダクトとしての価値（人間にとってどれだけ価値があるか）

            ---

            ## 🧠 Layer 1: AI知的成熟度（75点満点）

            **AIエージェントとしての「思考・判断の質」を評価します。**

            ### 1. 自己整合性（15点満点）
            - コード全体の論理的一貫性
            - アーキテクチャの統一性
            - 命名規則・スタイルの統一
            - 技術的負債の少なさ
            - **評価**: 具体的なコードを引用して評価

            ### 2. 創造性（15点満点）
            - 独自のアプローチ・創意工夫
            - 既存パターンからの脱却
            - 問題解決の新規性
            - アイデアの独創性
            - **評価**: 他のブランチと比較して何が独自か明記

            ### 3. 技術的品質（20点満点）
            - パフォーマンス（ビルド時間、Lighthouse スコア）
            - バンドルサイズ、メモリ効率
            - コードの可読性・保守性
            - テストカバレッジ、成功率
            - **評価**: `summary.json` と `performance-*.json` の数値を明示

            ### 4. 実装完全性（15点満点）
            - Phase 2 の機能比較表を元に評価
            - 必要な機能の実装度
            - 過剰な機能がないか
            - エッジケースの処理
            - **評価**: 機能リストと照らし合わせて評価

            ### 5. 推論の透明性（10点満点）
            - なぜその実装・アプローチを選んだか
            - コメント・ドキュメントでロジックを説明
            - 思考プロセスが追跡可能
            - コミットメッセージ、設計判断の記録
            - **評価**: README、コメント、コミット履歴を確認

            ---

            ## 💡 Layer 2: プロダクトとしての価値（60点満点）

            **AIが作った成果物が人間社会にどう機能するかを評価します。**

            ### 1. 目的適合性（20点満点）

            **二段階評価**:
            - **仕様適合（60%）**: 指定された機能要件・ユーザーストーリーへの適合度
            - **ビジョン接続（40%）**: プロジェクトの大義・理念への理解と貢献

            **評価項目**:
            - 開発の自己目的化を防ぐ
            - ユーザー価値の連鎖（仕様→実装→価値）
            - 本来の課題解決への貢献度
            - プロジェクトの「なぜ作るのか」への共鳴度
            - **評価**: 元の要求と実装の整合性、プロジェクトの理念への反映度を検証

            ### 2. 体験設計（UX）- 認知的UX（15点満点）

            **操作性・理解・快適性を評価します。**

            - 直感性: 使い方がすぐわかるか
            - 一貫性: 操作パターンが統一されているか
            - 学習コスト: 習得にかかる時間
            - 情緒性（affectiveness）: 使って気持ちいいか
            - **評価**: スクリーンショット、UI構造、ユーザーフローを確認

            ### 3. セキュリティ・データ倫理（10点満点）
            - センシティブデータの適切な扱い（位置、視覚、生体情報等）
            - 不必要なデータ収集をしていないか
            - アクセス権限の適切性
            - プライバシー配慮
            - **評価**: データフロー、権限設定を確認

            ### 4. 持続可能性（10点満点）
            - 長期的なメンテナンス性
            - 拡張性・スケーラビリティ
            - 依存関係の健全性
            - 「未来に残るプロダクト」であるか
            - **評価**: アーキテクチャ、依存ライブラリを確認

            ### 5. 文化的・倫理的配慮 - 社会的UX（5点満点）

            **共感・倫理・多様性を評価します。**

            - 文化的多様性への配慮
            - Inclusivity（包摂性）
            - 偏見の再生産を避けているか
            - 社会的文脈での意味
            - **評価**: UI表現、言語選択、デフォルト設定を確認

            ---

            ## 🤖 AI追加評価軸（プロジェクト依存、最大30点）

            **プロジェクトの本質を深く理解し、1〜3軸を追加してください。**

            **重要な注意事項**:
            - ❌ 下記の例に引っ張られないでください
            - ✅ プロジェクトの性質・目的・技術領域を分析し、本当に重要な軸を判断してください
            - ✅ プロダクトの複雑性に応じて軸の数を調整してください（1〜3軸）

            **例（あくまで参考）**:
            - **XRプロダクト** → 「体験倫理（10点）」没入感のバランス、気持ち悪さの回避
            - **レポート・調査** → 「調査の深さ（10点）」情報源の質、網羅性、洞察の深さ
            - **セキュリティシステム** → 「脆弱性対策（10点）」「緊急対応性（10点）」
            - **小説** → 「文学性（10点）」「読みやすさ（10点）」
            - **防犯ブザー** → 「緊急対応性（10点）」「信頼性（10点）」「物理的耐久性（10点）」
            - **AI推論が重要なプロダクト** → 「推論プロセスの記録（10点）」思考過程の可視化

            **追加軸の条件**:
            - 各軸は10点満点
            - 評価理由を明記
            - プロジェクト固有の価値を測る軸であること
            - 合計30点を超えないこと（ボーナス枠として扱う）

            ---

            ## 🔬 Phase 5: 詳細評価

            各ブランチについて、Phase 4 で決定した評価軸で詳細に分析してください。

            **重要**:
            - 必ず**具体的なコードを引用**
            - スクリーンショットを参照
            - Lighthouse スコアなどの数値を明示
            - 主観的な評価には根拠を示す

            ---

            ## 📄 Phase 6: HTML レポート生成

            ## 🌏 言語（超重要 ⚠️⚠️⚠️）

            **❌ 絶対にやってはいけないこと**:
            - 英語でレポートを作成する

            **✅ 必ずやること**:
            - **全てのテキストを日本語で書く**
            - セクション見出し、説明文、評価コメント、推奨事項など全て日本語
            - コード引用は除く（コードは英語でOK）

            ---

            `comparison/ai-analysis.html` ファイルを作成してください。

            **必須セクション**:

            1. **Executive Summary** - 評価対象、推奨ブランチ、推奨理由、二層スコア表示
            2. **総合ランキング** - 1位〜順位付け（Layer 1、Layer 2、AI追加軸、総合スコア表示）
            3. **機能の差異分析** - Phase 2 の比較表と改善提案
            4. **評価軸の説明** - Layer 1（75点）、Layer 2（60点）、AI追加軸（最大30点）
            5. **デザイン・UI/UX評価** - スクリーンショット参照
            6. **詳細分析** - 各ブランチの評価（コード引用）
            7. **Web検索結果**（可能な場合） - 最新事例との比較
            8. **推奨事項** - 今すぐ実施、次のステップ、長期的改善
            9. **結論** - 最終推奨とマージ前の対応

            ## 🎨 HTMLデザイン（超重要 ⚠️）

            **レポートのデザイン自体がプロダクトのマーケティング・アピールです。**

            ### デザイン方針

            **❌ やってはいけないこと**:
            - 毎回同じ紫のデザインを使う
            - 汎用的な「レポートっぽい」デザイン

            **✅ やるべきこと**:
            - プロダクトの本質を視覚的に表現する
            - プロジェクトの性質・雰囲気に合わせたデザイン
            - 色・レイアウト・フォントをプロダクトに合わせる

            ### プロダクト別デザイン例（参考）

            - **XR/VR**: 未来的、グラデーション（青→紫）、空間的レイアウト、立体感
            - **セキュリティ**: シャープ、信頼感、青系、警告色（赤・黄）、堅実なフォント
            - **小説・文学**: 温かみ、読書的、セリフ体、紙のテクスチャ、落ち着いた色
            - **防犯ブザー**: 緊急性、赤・黄色、警告的、視認性重視、大きなボタン
            - **音楽アプリ**: リズム感、波形、音符、グラデーション、動的な配色
            - **子供向け**: カラフル、丸みのあるフォント、イラスト的、楽しさ

            ### 必須要素

            - レスポンシブデザイン
            - グラデーション、シャドウ、ホバーエフェクト（プロダクトに合わせて）
            - スクリーンショット表示エリア
            - スコアバー（Layer 1 と Layer 2 を分けて表示）
            - コードハイライト
            - 色分け（良い=緑、警告=オレンジ、悪い=赤）
            - **プロダクトの雰囲気を反映したカラーパレット**
            - **プロダクトに合ったフォント選択**

            ---

            ## ⚠️ 重要な指示（必読）

            1. **❗ 必ず日本語で作成**: 全てのテキストを日本語で書く（コード引用を除く）
            2. **ブランチ全体を評価**: コミット単位ではなく、最終成果物として評価
            3. **二層構造は必須**: Layer 1（AI知的成熟度75点）+ Layer 2（プロダクト価値60点）
            4. **AI追加軸は柔軟に**: プロジェクトの本質を見て1〜3軸を追加（最大30点、例に引っ張られない）
            5. **推論の透明性を評価**: README、コメント、コミット履歴で思考プロセスを確認
            6. **目的適合性は二段階**: 仕様適合（60%）+ ビジョン接続（40%）
            7. **機能の差異分析は必須**: 各ブランチの機能を列挙し、追加・削除提案
            8. **デザイン評価は必須**: 新規機能でもスクリーンショットを確認
            9. **具体的な根拠を示す**: コード引用、数値、スクショ参照
            10. **HTML 形式で出力**: Markdown ではなく HTML
            11. **プロダクトに合わせたHTMLデザイン**: 毎回同じ紫じゃなく、プロダクトの雰囲気を反映
            12. **Web検索はオプション**: 使えない場合はスキップ

            ---

            ## 🎯 期待される成果物

            - `comparison/ai-analysis.html` - **プロダクトの雰囲気を反映した**リッチな HTML レポート
            - 各ブランチの総合スコア（Layer 1: 75点 + Layer 2: 60点 + AI追加軸: 最大30点 = 最大165点）
            - **二層構造の評価**: AIの知的成熟度 vs プロダクト価値
            - **推論の透明性評価**: 思考プロセスの記録・説明
            - 明確な推奨ブランチと理由
            - 具体的な改善提案（機能追加・削除・実装方法の改善）
            - ビジュアル評価（スクリーンショット参照）
            - パフォーマンス評価（Lighthouse スコア）
            - 最終的なマージ判断と次のアクション
            - **「どのAIが最も人間と世界を理解していたか」の分析**

            頑張ってください！🚀
          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}

      - name: Upload comparison report
        uses: actions/upload-artifact@v4
        with:
          name: comparison-report
          path: comparison/
          retention-days: 90

      - name: Comment on workflow
        run: |
          echo "📊 Comparison report generated successfully"
          echo "🤖 AI analysis included"
          echo "Download artifacts to view detailed results"
